# -*- coding: utf-8 -*-
"""NN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qg-vqhy844-8xmfLgdnkuU9NCWjis2ks

## IMPORT PART
"""

import random as r
import numpy as np
import pandas as pd
import math as m
import dataloader as load
from pathlib import Path
import matplotlib.pyplot as plt

"""## DEF PART"""

# ReLu
def relu(x):
    return np.maximum(0, x)


# derivative of ReLU
def derivative(x):
    return x > 0


# histogram(전처리)
def histogram(img):
    img256 = np.round(img * 255)

    histo, bin = np.histogram(img256, 784, [0, 256])

    cdf = histo.cumsum()
    # imgshow(img256)

    cdf = (cdf - cdf.min()) * 256 / (cdf.max() - cdf.min())

    img = cdf[img256.astype(int)]
    img = img / 255

    return img


# test
def imgshow(image):
    for i in range(28):
        for j in range(28):
            print('{:3}'.format(round(image[i * 28 + j])), end="  ")
        print("")


# cross entropy
def ce(lst1, lst2):
    value = np.max(lst1 * lst2)
    # print(lst1)
    # print(value)
    return -np.log(value)


# softmax ( 안정 )
def softmax(lst):
    maximum = np.max(lst)
    expo = np.exp(lst - maximum)
    sum = np.sum(expo)
    return expo / sum

# 흠..
def confu(lst):
  for i in range(10):
    sum = np.sum(lst[i])
    lst[i]=lst[i]/sum
  return 0

def linforward(ilayer, iweight):
    return np.dot(ilayer, iweight)


def reluforward(ilayer):
    return relu(ilayer)


def linbackward(ilayer, iweight, ograd):
    yield np.dot(ograd, iweight.T)
    yield np.dot(np.array([ilayer]), ograd.T)


def relubackward(ilayer, ograd):
    grad = ilayer > 0
    return grad * ograd

"""##Lin Class"""

class lin:
    def __init__(self, isize, osize, alpha=0.1):
        """
        s = m.sqrt(2/isize)
        n = np.random.randn(isize,osize)
        self.weights = n*s
        """
        self.weights = np.random.normal(0, m.sqrt(2 / isize), (isize, osize))
        # self.weights = (np.random.rand(isize, osize)-0.5)/isize*2
        # self.weights = np.random.uniform(-m.sqrt(6/isize),m.sqrt(6/isize), (isize, osize))
        self.lrate = alpha
        self.bias = np.zeros(osize) / isize

    def forward(self, inp, dropout=0.5):
        # return np.dot(inp,self.weights)
        if dropout == 0:
            return np.dot(inp, self.weights) 
        m, n = np.shape(self.weights)
        temp = np.random.rand(m, n)
        temp2 = temp > dropout
        # print(np.shape(temp2))
        # print(temp2.reshape(784*400))

        temp2 = temp2 * self.weights * (1 / (1 - dropout))
        return np.dot(inp, temp2) + self.bias

    # 이게뭐죠?
    def firstback(self, ograd, layer):
        ngrad = np.dot(np.array(ograd).T, np.array([layer]))
        self.weights -= self.lrate * ngrad.T
        return ngrad

    def backward(self, inp, ograd):
        # print(np.shape(inp), "inp",  np.shape(ograd), "ograd", np.shape(self.weights), "weights")
        igrad = np.dot(ograd, self.weights.T)
        wgrad = np.dot(np.array([inp]).T, ograd)
        # print(np.shape(wgrad))
        self.weights -= self.lrate * wgrad
        return igrad

"""## NN Class"""

class NN:
    def __init__(self, ilsize, hlsize, olsize, alpha):
        self.il = np.zeros(ilsize)
        self.sl = np.zeros(hlsize)
        self.slr = np.zeros(hlsize)
        self.tl = np.zeros(olsize)
        self.tlr = np.zeros(olsize)
        self.ol = np.zeros(10)
        self.lrate = alpha
        self.fw = lin(ilsize, hlsize, alpha)
        self.sw = lin(hlsize, olsize, alpha)
        self.tw = lin(olsize, 10, alpha)
        self.softmax = np.zeros(10)
        self.ograd = np.zeros(10)

    def __getitem__(self, fw):
        return fw

    def forward(self, input, dropout=0.5):
        self.il = input
        self.sl = self.fw.forward(input, dropout)
        self.slr = relu(self.sl)
        self.tl = self.sw.forward(self.slr, dropout)
        self.tlr = relu(self.tl)
        self.ol = self.tw.forward(self.tlr, dropout)
        self.softmax = softmax(self.ol)
        return self.softmax

    def calograd(self, ans):
        self.ograd = self.softmax - ans
        return 0

    def backward(self):
        temp = self.ograd
        # print(np.shape(self.ograd))
        """
        self.ograd = self.tw.firstback(self.ograd, self.tlr)
        self.ograd = relubackward(self.tlr, self.ograd)

        self.ograd = np.dot(self.ograd.T, temp.T).T
        #print(np.shape(self.ograd))
        """
        self.ograd = self.tw.backward(self.tl, self.ograd)

        self.ograd = relubackward(self.tlr, self.ograd)

        self.ograd = self.sw.backward(self.sl, self.ograd)
        # print(np.shape(self.ograd))
        self.ograd = relubackward(self.slr, self.ograd)
        # print(self.ograd*768,"ograd")
        self.ograd = self.fw.backward(self.il, self.ograd)

    def train(self, image, label):
        pass

    # for image 1
    def train_and_test(self, traini, trainl, testi, testl, dropout=0):
        testloss = 0
        trainloss = ce(self.forward(traini, dropout), trainl)
        self.calograd(trainl)
        self.backward()

        testloss = ce(self.forward(testi, dropout), testl)

        return (trainloss, testloss)

    # for image 2, 3
    def test(self, testi, testl, matrix, topthree, topthreeimg):
        sm = softmax(self.forward(testi, dropout=0))
        crossentropy = ce(sm, testl)
        predict = np.argmax(sm)
        real = np.argmax(testl)
        if predict == real:
          amax = np.argmin(topthree[predict])
          if topthree[predict][amax] < sm[predict]:
            topthree[predict][amax] = sm[predict]
            topthreeimg[predict][amax] = testi
          pass
        matrix[real][predict] += 1

"""## HYPERPARAMETER PART"""

# layer size
    ilsize = 784
    hlsize = 200
    olsize = 100
    # 784 400 300 ->70%

    # alpha = learning rate
    alpha = 0.005

    # batch_size
    batch_size = 1

    # epoch
    epoch = 50
    # suffle?
    suffle = True

    # NN DataSet
    NeuralNet = NN(ilsize, hlsize, olsize, alpha)

    # data 불러오기, data size는 28*28!
    traindata = load.Dataloader(Path.cwd(), True, batch_size, suffle)
    trainsize = len(traindata)
    # print(trainsize)
    testdata = load.Dataloader(Path.cwd(), False, batch_size, suffle)
    testsize = len(testdata)

    # 실험용으로만..

    # test용 data section
 

    print("epoch: %d, learning rate: %f, size: %d %d %d" % (epoch, alpha, ilsize, hlsize, olsize))

"""## TRAIN PART"""

trainloss=[]
testloss=[]
for i in range(epoch):
  corcount = 0
  wrongcount = 0
  lossinepoch = 0
  count = 0
  cross = 0
  tcross = 0
  for trainimage, trainlabel in traindata:
    img = trainimage.reshape(784)
    img = histogram(img)
    sm = NeuralNet.forward(img, dropout=0)
    loss = ce(sm, trainlabel)
    cross+=loss
    #print(loss)
    count+=1
    if np.argmax(sm) == np.argmax(trainlabel):
        corcount += 1
    else:
        wrongcount +=1
    #print(corcount/wrongcount)
    NeuralNet.calograd(trainlabel)
    NeuralNet.backward()
    #if count % 6000 == 0:
        #print(count/600, "%, current accuracy is ", '{:5}'.format(corcount/(wrongcount+corcount)*100)," %")
        #print(NeuralNet.sw.weights.reshape(hlsize*olsize)*hlsize)
        #print(NeuralNet.tw.weights.reshape(olsize*10)*olsize)
  for testimage, testlabel in testdata:
    img = testimage.reshape(784)
    img = histogram(img)
    sm = NeuralNet.forward(img, dropout=0)
    tcross += ce(sm, testlabel)
  trainloss.append(cross/60000)
  testloss.append(tcross/10000)
  print(i + 1, "epoch ended, current accuracy is ", corcount / 60000, "\n","avg. trainloss is ", cross / 60000 , "\n", "avg.testloss is", tcross/10000)

"""##IMAGE"""

t = np.arange(0, epoch, 1)
print(trainloss, testloss)
plt.subplots_adjust(left=0.125, bottom=-0.5, right=0.9, top=0.9, wspace=0.2, hspace=0.2)
plt.subplot(3,1,1)
plt.plot(t, trainloss)
plt.subplot(3,1,2)
plt.plot(t,testloss,'r')
plt.subplot(3,1,3)
plt.plot(t,trainloss,t,testloss,'r')


plt.show()

t = np.arange(0, epoch, 1)
plt.plot(t, trainloss, marker='.', c= 'r', label = "train-set loss")
plt.plot(t, testloss, marker='.', label = "test-set loss")
plt.legend(loc='upper right')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('cross entropy loss')
plt.show()

"""## TEST PART"""

# test
testcount = 0
matrix = np.zeros((10,10))
topthree = np.zeros((10,3))
topthreeimg = [[0,0,0] for _ in range(10)]
print(topthreeimg[0][1])
for testimage, testlabel in testdata:
    img = histogram(testimage.reshape(784))
    NeuralNet.test(img, testlabel, matrix, topthree,topthreeimg)
    sm = NeuralNet.forward(img, dropout=0)
    if np.argmax(sm) == np.argmax(testlabel):
        testcount += 1
        # print(corcount/wrongcount)

"""## Statics"""

# accuracy
print(testcount)
print(testsize)
print(testcount / testsize)
 
fig, ax = plt.subplots(1,1)

#plt.legend(loc='lower right')
matrix=matrix.T
row = (0,1,2,3,4,5,6,7,8,9)
col = (0,1,2,3,4,5,6,7,8,9)
confu(matrix)
matrix=matrix.T
matrix =  np.round(matrix, 2)
ax.axis('tight')
ax.axis('off')
table = ax.table(cellText=matrix, rowLabels = row, colLabels = col, loc= "center", fontsize=30, colWidths =[0.1]*10)
table.scale(1,3)
plt.xlabel('Label')
plt.ylabel('Predictions')
plt.show()

fig = plt.subplots(10,3)
for i in range(10):
  for j in range(3):
    plt.axis('off')
    plt.subplot(10,3,3*i+j+1)
    plt.imshow(topthreeimg[i][j].reshape(28,28))
print(topthree)